{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do new env \n",
    "#Tested on Python >=3.9\n",
    "import pandas as pd\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "import time\n",
    "key= 'Your-Key-Here'\n",
    "client = OpenAI(api_key=key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_stratified_sample(df, difficulty, specialty=None):\n",
    "    # Use exception handling to catch errors depending on your use Case\n",
    "    # Check if necessary columns are present in DataFrame\n",
    "    required_columns = {'Specialty', 'CorrectResponse', 'Questions'}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        print(\"Error: Missing required columns in DataFrame. Required columns are: 'Specialty', 'CorrectResponse', 'Questions'\")\n",
    "        # raise ValueError(\"Missing required columns in DataFrame. Required columns are: 'Specialty', 'CorrectResponse', 'Questions'\")\n",
    "        return None\n",
    "\n",
    "    # Handling different difficulty levels\n",
    "    if difficulty == 'Alpha':\n",
    "        if specialty is None:\n",
    "            print(\"Error: Specialty must be specified for difficulty 'Alpha'\")\n",
    "            # raise ValueError(\"Specialty must be specified for difficulty 'Alpha'\")\n",
    "            return None\n",
    "        if specialty not in df['Specialty'].unique():\n",
    "            print(f\"Error: Specialty '{specialty}' not found in DataFrame\")\n",
    "            # raise ValueError(f\"Specialty '{specialty}' not found in DataFrame\")\n",
    "            return None\n",
    "        \n",
    "        # Filter DataFrame for the given specialty\n",
    "        specialty_df = df[df['Specialty'] == specialty]\n",
    "        correct_responses = specialty_df['CorrectResponse']\n",
    "        questions = specialty_df['Questions']\n",
    "        return specialty_df, correct_responses, questions\n",
    "\n",
    "    elif difficulty == 'Beta':\n",
    "        # Perform stratified sampling to retrieve 50% from each specialty\n",
    "        try:\n",
    "            selected = df.groupby('Specialty', group_keys=False).apply(lambda x: x.sample(frac=0.5, random_state=42))\n",
    "        except ValueError as e:\n",
    "            print(f\"Error during sampling: {e}\")\n",
    "            return None\n",
    "        \n",
    "        correct_responses = selected['CorrectResponse']\n",
    "        questions = selected['Questions']\n",
    "        return selected, correct_responses, questions\n",
    "\n",
    "    elif difficulty == 'Production':\n",
    "        # Return the entire DataFrame, ie the Full Dataset\n",
    "        questions = df['Questions']\n",
    "        correct_responses = df['CorrectResponse']\n",
    "        return df, correct_responses, questions\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Invalid difficulty level. Expected 'Alpha', 'Beta', or 'Production'\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_assistant(user_input,model='OpenAI'):\n",
    "    # Custom tools can be defined here (if needed)\n",
    "  if model =='OpenAI':\n",
    "      tools = [\n",
    "          {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\":\"Assistant\",\n",
    "              \"description\": '''On Multiple choice Quiz choose correct responses:(ONLY THE CORRECT LETTERS and no spaces and no other associated Text. If more than one letter then add a dash- between letters).\n",
    "              For example if question is \"Which of the following are programming languages? A. Python B. HTML C. JavaScript D. SQL E. CSS. then your response should be:A-C-D'''\n",
    "            }\n",
    "          }\n",
    "        ]\n",
    "        \n",
    "      \n",
    "      # Check if there's an existing conversation history\n",
    "      if 'history' not in chat_with_assistant.__dict__:\n",
    "          chat_with_assistant.history = []\n",
    "\n",
    "      # Append the user's message to the history\n",
    "      chat_with_assistant.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "      # Generate a response from the assistant\n",
    "      completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-0613\",\n",
    "        messages=chat_with_assistant.history,\n",
    "        # stream=True,\n",
    "        tools=tools,\n",
    "        tool_choice=\"none\",\n",
    "      )\n",
    "      full_response = completion.choices[0].message.content\n",
    "      \n",
    "      chat_with_assistant.history = []\n",
    "      #Here add to get\n",
    "      return full_response\n",
    "  else:\n",
    "      #  Add your model logic here \n",
    "      return \"Your Model's response\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>CorrectResponse</th>\n",
       "      <th>Specialty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which of the following is not identified as a ...</td>\n",
       "      <td>D</td>\n",
       "      <td>Psychiatry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which term refers to the likelihood of identif...</td>\n",
       "      <td>E</td>\n",
       "      <td>Psychiatry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the probability of a patient inheritin...</td>\n",
       "      <td>A</td>\n",
       "      <td>Psychiatry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is the term used to describe a situation where...</td>\n",
       "      <td>B</td>\n",
       "      <td>Psychiatry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which of the following is not a characteristic...</td>\n",
       "      <td>B</td>\n",
       "      <td>Psychiatry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Questions CorrectResponse  \\\n",
       "0  Which of the following is not identified as a ...               D   \n",
       "1  Which term refers to the likelihood of identif...               E   \n",
       "2  What is the probability of a patient inheritin...               A   \n",
       "3  Is the term used to describe a situation where...               B   \n",
       "4  Which of the following is not a characteristic...               B   \n",
       "\n",
       "    Specialty  \n",
       "0  Psychiatry  \n",
       "1  Psychiatry  \n",
       "2  Psychiatry  \n",
       "3  Psychiatry  \n",
       "4  Psychiatry  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting up the collection of questions and response handling\n",
    "import pandas as pd\n",
    "# load from local file if downloaded or use hGGimport.py to load from huggingFace and add your logic\n",
    "Cognet=pd.read_csv('cgnetMD1_0.csv') \n",
    "# Sample the DataFrame and choose difficulty level as 'Alpha', 'Beta', or 'Production'\n",
    "# If Alpha is chosen, specify the specialty as well -At version 1.0 available specialties are:  'Dermatology', 'Psychiatry', 'Neurology', 'Endocrinology', 'Pulmonology-Respiratory'\n",
    "sampled_df, correct_responses,questions = get_stratified_sample(Cognet,'Beta')\n",
    "\n",
    "# Print for testing purposes-Consider computational cost\n",
    "# print(sampled_df['Specialty'].value_counts())\n",
    "# print(correct_responses.tolist())\n",
    "# print(questions.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if it works \n",
    "chat_with_assistant(\"Which of the following are programming languages? A. Python B. HTML C. JavaScript D. SQL E. CSS. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on Size of dataframe add delay as sleep to avoid rate limit or buffer overflow\n",
    "# Could be adjusted to handle the rate limit via a try except block and sleep for a few seconds, but it should work fine testing it out\n",
    "#Adjust Sleep time as needed depending on your model and rate limit\n",
    "collectResponses=[]\n",
    "for i in questions:\n",
    "    Response = chat_with_assistant(i)\n",
    "    time.sleep(1)\n",
    "    print(Response)\n",
    "    collectResponses.append(Response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Clean up quiz</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Clean up to ensure the quiz list is in the correct format Adjust as required based on the response from the assistant\n",
    "def clean_and_order_quiz_list(quiz_list, separator='-'):\n",
    "    cleaned_list = []\n",
    "    counter =0\n",
    "\n",
    "    for item in quiz_list:\n",
    "        # Check if the item contains letters with associated text\n",
    "        if re.search(r'[A-Z]\\.', item):\n",
    "            # Extract letter choices and sort them\n",
    "            choices = re.findall(r'([A-Z])\\.', item)\n",
    "            #Count the instances where LLM provided a wrong response structure\n",
    "            counter+=1\n",
    "            # Sort the choices\n",
    "            print(counter)\n",
    "            sorted_choices = sorted(choices)\n",
    "            # Join the sorted choices with dashes\n",
    "            cleaned_item = separator.join(sorted_choices)\n",
    "        else:\n",
    "            # Ensure item is in the correct format (choices separated by dashes)\n",
    "            # and sort them if it's a single letter\n",
    "            choices = sorted(item.split(separator))\n",
    "            cleaned_item = separator.join(choices)\n",
    "\n",
    "        cleaned_list.append(cleaned_item)\n",
    "\n",
    "    return cleaned_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_list = collectResponses\n",
    "print(collectResponses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COGNET-MD v.1.0 - Score Responses\n",
    "def score_responses(user_responses, correct_responses):\n",
    "    score = 0.0\n",
    "    \n",
    "   \n",
    "    # to distict between normal and hard rules of conduct are either suggestive or not suggestive\n",
    "    PartialCredit=0\n",
    "    FullCredit=0\n",
    "    penaltyCount = 0\n",
    "    \n",
    "    partialCreditIndex = []\n",
    "    fullCreditIndex = []\n",
    "    for index, (user_response, correct_response) in enumerate(zip(user_responses, correct_responses)):\n",
    "            # Split the responses into sets of choices\n",
    "        user_choices = set(user_response.split('-'))\n",
    "        correct_choices = set(correct_response.split('-'))\n",
    "            \n",
    "            # Calculate correct and incorrect choices\n",
    "        correct_selected = user_choices & correct_choices\n",
    "        incorrect_selected = user_choices - correct_choices\n",
    "        \n",
    "        #count correct and incorrect selectec\n",
    "        \n",
    "        if correct_selected:\n",
    "            partialCreditIndex.append(index)\n",
    "                # Partial credit for selecting at least one correct answer\n",
    "            PartialCredit+=1\n",
    "            score += 0.5\n",
    "            \n",
    "        if correct_selected == correct_choices and not incorrect_selected:\n",
    "                # Full credit for selecting all correct answers and no incorrect ones\n",
    "            fullCreditIndex.append(index)\n",
    "            FullCredit+=1\n",
    "            score += 0.5  # additional 0.5 for making it full 1 point in total\n",
    "            \n",
    "            # Deduct points for incorrect answers\n",
    "        if incorrect_selected:\n",
    "            score -= 0.5 * len(incorrect_selected)\n",
    "            penaltyCount += len(incorrect_selected)  # Count each incorrect choice as a penalty\n",
    "\n",
    "    return PartialCredit,FullCredit,score,partialCreditIndex,fullCreditIndex,penaltyCount\n",
    "\n",
    "def calculate_accuracy(user_responses, correct_responses):\n",
    "    total_questions = len(correct_responses)\n",
    "    \n",
    "    max_score = total_questions  # Each question is worth 1 point if answered correctly\n",
    "    PartialCredit,FullCredit,actual_score,partialCreditIndex,fullCreditIndex,penaltyCount = score_responses(user_responses, correct_responses)\n",
    "    print(actual_score)\n",
    "    accuracy = (actual_score/ max_score) * 100\n",
    "    return PartialCredit,FullCredit,accuracy,partialCreditIndex,fullCreditIndex,penaltyCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_quiz_list=clean_and_order_quiz_list(quiz_list)\n",
    "print(cleaned_quiz_list)\n",
    "print(correct_responses.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PartialCredit,FullCredit,accuracy,partialCreditIndex,fullCreditIndex,penaltyCount= calculate_accuracy(cleaned_quiz_list, correct_responses.tolist())\n",
    "print(f\"Partial Credits were equal to: {PartialCredit*0.5}, for at least one correct choice in Multiple choice questions \")\n",
    "print(f\"Total partially answered questions: {PartialCredit}\")\n",
    "print(\"Full Credit, all correct in a Multiple choice question:\", FullCredit)\n",
    "\n",
    "print(f\"Penalties given {penaltyCount} times for a total of {penaltyCount*0.5} points being deducted\")\n",
    "print(f\"Accuracy Based on COGNET-MD scoring system:\\n {accuracy}% for a total of {len(cleaned_quiz_list)} questions\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
